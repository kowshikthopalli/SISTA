{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52e5c093",
   "metadata": {},
   "source": [
    "# Generative Augmentation\n",
    "\n",
    "This notebook demonstrates the different augmentation techniques discussed in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f081cf-ee13-4a33-99f6-9ecf00a71957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "torch.backends.cudnn.benchmark = True\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.utils import save_image\n",
    "from util import *\n",
    "from PIL import Image\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "from torch import nn, autograd, optim\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import *\n",
    "from e4e_projection import projection as e4e_projection\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "\n",
    "except ImportError:\n",
    "    wandb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7768b29b-09cc-4a89-9acc-087624d38864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent(paths, device=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Given list of images projects each image into styleGANs latent space using PSP\n",
    "    :param \n",
    "        paths:  list of images\n",
    "    :return\n",
    "        imgs: torch array of the inpit images\n",
    "        latents: torch array of latent codes\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((1024, 1024)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    "    )\n",
    "    if type(paths) == list(): paths = [paths]\n",
    "    \n",
    "    imgs, latents = [], []\n",
    "    for path in paths:\n",
    "        assert os.path.exists(path), f\"{path} does not exist!\"\n",
    "            \n",
    "        img = Image.open(path).convert('RGB')\n",
    "        \n",
    "        latent = e4e_projection(img, device=device)\n",
    "        imgs.append(transform(img))\n",
    "        latents.append(latent)\n",
    "    \n",
    "    latents = torch.stack(latents, 0).squeeze(1).to(device)\n",
    "    imgs = torch.stack(imgs, 0).to(device)\n",
    "    \n",
    "    return imgs, latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bc85a7-dc43-472f-8252-33fa3a824c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_StyleGAN(original_generator, discriminator, styleA_img, latents):\n",
    "\n",
    "    alpha =  0\n",
    "\n",
    "    preserve_color = False\n",
    "    num_iter = 300\n",
    "\n",
    "    # del generatorA, generatorB, trans_func\n",
    "    # to be finetuned generator\n",
    "    generator = deepcopy(original_generator)\n",
    "    \n",
    "    optimizer = optim.Adam(generator.parameters(), lr=2e-3, betas=(0, 0.99))\n",
    "\n",
    "    # Which layers to swap for generating a family of plausible real images -> fake image\n",
    "    if preserve_color:\n",
    "        id_swap = [9,11,15,16,17]\n",
    "    else:\n",
    "        id_swap = list(range(7, original_generator.n_latent))\n",
    "\n",
    "    # remove the comment if wanting to randomize the face image for the pair\n",
    "    if latents == None:\n",
    "        latents = generator.get_latent(\n",
    "            torch.randn([latents.size(0), latents.size(1), latent_dim]).to(device))\n",
    "\n",
    "    pbar = tqdm(range(num_iter))\n",
    "    for idx in pbar:\n",
    "        # Sample a random w from styleGAN latent space\n",
    "        rand_w = original_generator.get_latent(\n",
    "            torch.randn([latents.size(0), latent_dim]).to(device)).unsqueeze(1).repeat(1, original_generator.n_latent, 1)\n",
    "        # Clone the W+ of StyleA obtained from PSP\n",
    "        in_latent = latents.clone()\n",
    "\n",
    "        # Replace the last layers of in_latent+ with transformed rand_w\n",
    "        in_latent[:, id_swap] = alpha*in_latent[:, id_swap] + (1-alpha)*rand_w[:, id_swap]\n",
    "\n",
    "        # Generate styleA\n",
    "        gen_imgA = generator(in_latent, input_is_latent=True)\n",
    "\n",
    "        # Obtain the features for the discriminator\n",
    "        with torch.no_grad():\n",
    "            real_feat_A = discriminator(styleA_img)\n",
    "        fake_feat_A = discriminator(gen_imgA)\n",
    "\n",
    "        # # Compute L1 feature loss of (realA, genA) and (realB, genB)\n",
    "        loss_disc_A = sum([F.l1_loss(a, b) for a, b in zip(real_feat_A, fake_feat_A)])/len(fake_feat_A)\n",
    "\n",
    "        loss = loss_disc_A\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()            \n",
    "        \n",
    "        pbar.set_description(\n",
    "            (f\" loss_disc_A: {loss_disc_A.item():.4f};\")\n",
    "            )\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d879b193",
   "metadata": {},
   "source": [
    "Load the stleGAN generator for different domains, if not saved trained finetune styleGAN for different domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc071566-8787-4f14-9901-a483619529c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_generators(original_generator, discriminator, model_A_path='models/stylegan2-pencil_sketch.pt', model_B_path='models/stylegan2-water_color.pt', model_C_path='models/stylegan2-color_sketch.pt'):\n",
    "    \"\"\"\n",
    "    Given list of images projects each image into styleGANs latent space using PSP\n",
    "    :param \n",
    "        paths:  list of images\n",
    "    :return\n",
    "        imgs: torch array of the inpit images\n",
    "        latents: torch array of latent codes\n",
    "    \"\"\"\n",
    "    if os.path.exists(model_A_path):\n",
    "        print(f'Loaded {model_A_path}')\n",
    "        ckpt = torch.load(model_A_path, map_location=lambda storage, loc: storage)\n",
    "        generatorA = deepcopy(original_generator)\n",
    "        generatorA.load_state_dict(ckpt[\"g_ema\"], strict=False)\n",
    "    else:\n",
    "        imgs = ['data/style_ref/celebA_ref/sketch_147.jpg'] # Ref image of domain A\n",
    "        sketch_ref, sketch_w = get_latent(imgs, device)\n",
    "        generatorA = finetune_StyleGAN(original_generator, discriminator, sketch_ref, sketch_w)\n",
    "        torch.save({\"g_ema\": generatorA.state_dict()}, model_A_path)\n",
    "\n",
    "    if os.path.exists(model_B_path):\n",
    "        print(f'Loaded {model_B_path}')\n",
    "        ckpt = torch.load(model_B_path, map_location=lambda storage, loc: storage)\n",
    "        generatorB = deepcopy(original_generator)\n",
    "        generatorB.load_state_dict(ckpt[\"g_ema\"], strict=False)\n",
    "    else:\n",
    "        imgs = ['data/style_ref/watercolor_147.jpg'] # Ref image of domain B\n",
    "        toon_ref, toon_w = get_latent(imgs, device)\n",
    "        generatorB = finetune_StyleGAN(original_generator, discriminator, toon_ref, toon_w)\n",
    "        torch.save({\"g_ema\": generatorB.state_dict()}, model_B_path)\n",
    "\n",
    "\n",
    "    if os.path.exists(model_C_path):\n",
    "        print(f'Loaded {model_C_path}')\n",
    "        ckpt = torch.load(model_C_path, map_location=lambda storage, loc: storage)\n",
    "        generatorC = deepcopy(original_generator)\n",
    "        generatorC.load_state_dict(ckpt[\"g_ema\"], strict=False)\n",
    "    else:\n",
    "        imgs = ['data/style_ref/celebA_ref/color_sketch147.jpg'] # Ref image of domain C   \n",
    "        toon_ref, toon_w = get_latent(imgs, device)\n",
    "        generatorC = finetune_StyleGAN(original_generator, discriminator, toon_ref, toon_w)\n",
    "        torch.save({\"g_ema\": generatorC.state_dict()}, model_C_path)\n",
    "\n",
    "    return generatorA, generatorB, generatorC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8951fb24-15fa-4469-bda8-3c2d6457b7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "    \n",
    "iteration=1000 #\"total training iterations\"\n",
    "batch_size=1 #\"No of pairs each training iteration\"\n",
    "size=1024 #\"image sizes for the model\"\n",
    "latent_dim=512 #\"StyleGAN latent code dim\"\n",
    "\n",
    "StyleGAN_ckpt='models/stylegan2-ffhq-config-f.pt' #\"path to the checkpoints to resume training\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955f9618-b302-4a8e-87f0-12cb64bc3700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original generator\n",
    "original_generator = Generator(size, latent_dim, 8).to(device)\n",
    "ckpt = torch.load(StyleGAN_ckpt, map_location=lambda storage, loc: storage)\n",
    "original_generator.load_state_dict(ckpt[\"g_ema\"], strict=False)\n",
    "mean_latent = original_generator.mean_latent(10000)\n",
    "\n",
    "# load discriminator for perceptual loss\n",
    "discriminator = Discriminator(size).eval().to(device)\n",
    "ckpt = torch.load(StyleGAN_ckpt, map_location=lambda storage, loc: storage)\n",
    "discriminator.load_state_dict(ckpt[\"d\"], strict=False)\n",
    "discriminator.eval()\n",
    "print(\"Loaded generator and discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921af347-9dec-45d7-b8ce-dedaf1da45d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generatorA, generatorB, generatorC = load_generators(original_generator, discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925ee1bf",
   "metadata": {},
   "source": [
    "## Evaluate the loaded generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d3bf9-0d02-49d7-bf6d-a71b48ae1633",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = 1\n",
    "\n",
    "z = torch.randn(test_samples, latent_dim, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generatorA.eval()\n",
    "    generatorB.eval()\n",
    "    generatorC.eval()\n",
    "\n",
    "    true_img = original_generator([z], input_is_latent=False)\n",
    "    # Generate styleA and styleB\n",
    "    gen_imgA = generatorA([z], input_is_latent=False)\n",
    "    gen_imgB = generatorB([z], input_is_latent=False)\n",
    "    gen_imgC = generatorC([z], input_is_latent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc99d04-34bf-4386-ba61-cc61a50e3297",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_A = torch.cat([true_img, gen_imgA], 0)\n",
    "recon_A = utils.make_grid(recon_A, normalize=True, range=(-1, 1), nrow=test_samples)\n",
    "display_image(recon_A, title='TrueA, ReconA')\n",
    "\n",
    "recon_B = torch.cat([true_img, gen_imgB], 0)\n",
    "recon_B = utils.make_grid(recon_B, normalize=True, range=(-1, 1), nrow=test_samples)\n",
    "display_image(recon_B, title='TrueB, GenB')\n",
    "\n",
    "recon_C = torch.cat([true_img, gen_imgC], 0)\n",
    "recon_C = utils.make_grid(recon_C, normalize=True, range=(-1, 1), nrow=test_samples)\n",
    "display_image(recon_C, title='TrueB, GenC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627f2534",
   "metadata": {},
   "source": [
    "## Generate target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff266178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(original_generator, generatorA, generatorB, generatorC, num_samples = 10000):\n",
    "\n",
    "    # data_dir = 'data/multi_domain_interpol_gen'\n",
    "    data_dir = '/mnt/e0e0461d-3972-4e84-bf9b-685ecd68ebce/datasets/GAN/multi_domain_interpol_gen_10K'\n",
    "    \n",
    "\n",
    "    os.makedirs(f'{data_dir}/orig/', exist_ok=True)\n",
    "    os.makedirs(f'{data_dir}/z/', exist_ok=True)\n",
    "\n",
    "    os.makedirs(f'{data_dir}/pencil_sketch/', exist_ok=True)\n",
    "    os.makedirs(f'{data_dir}/colorsketch/', exist_ok=True)\n",
    "    os.makedirs(f'{data_dir}/watercolor/', exist_ok=True)\n",
    "\n",
    "    for i in tqdm(range(num_samples)):\n",
    "        # Sample a random z from styleGAN latent space\n",
    "        z = torch.randn(1, latent_dim, device=device)\n",
    "        # z = torch.load(f'{data_dir}/z/{i}.pt')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            orig_img = original_generator([z], input_is_latent=False)\n",
    "            orig_img = utils.make_grid(orig_img[0], normalize=True, range=(-1, 1))\n",
    "            save_image(orig_img, f'{data_dir}/orig/{i}.png')\n",
    "\n",
    "\n",
    "            dom_A = generatorA([z], input_is_latent=False)\n",
    "            dom_A = utils.make_grid(dom_A[0], normalize=True, range=(-1, 1))\n",
    "            save_image(dom_A, f'{data_dir}/pencil_sketch/{i}.png')\n",
    "\n",
    "\n",
    "            dom_B = generatorB([z], input_is_latent=False)\n",
    "            dom_B = utils.make_grid(dom_B[0], normalize=True, range=(-1, 1))\n",
    "            save_image(dom_B, f'{data_dir}/watercolor/{i}.png')\n",
    "\n",
    "            dom_C = generatorC([z], input_is_latent=False)\n",
    "            dom_C = utils.make_grid(dom_C[0], normalize=True, range=(-1, 1))\n",
    "            save_image(dom_C, f'{data_dir}/colorsketch/{i}.png')\n",
    "            torch.save(z, f'{data_dir}/z/{i}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0e187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_data(original_generator, generatorA, generatorB, generatorC, num_samples=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae81024",
   "metadata": {},
   "source": [
    "## Pruning Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3381f011",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = 1\n",
    "\n",
    "z = torch.randn(test_samples, latent_dim, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generatorA.eval()\n",
    "    generatorB.eval()\n",
    "\n",
    "    true_img = original_generator([z], input_is_latent=False)\n",
    "    true_img_zero = original_generator([z], input_is_latent=False, percentile=50, mask_type='zeros')\n",
    "    # Generate styleA and styleB\n",
    "    gen_imgA_zeros = generatorA([z], input_is_latent=False, percentile=50, mask_type='zeros')\n",
    "    gen_imgB_zeros = generatorB([z], input_is_latent=False, percentile=50, mask_type='zeros')\n",
    "    \n",
    "    gen_imgA = generatorA([z], input_is_latent=False)\n",
    "    gen_imgB = generatorB([z], input_is_latent=False)\n",
    "    gen_imgC = generatorC([z], input_is_latent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59ee0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_T = torch.cat([true_img, true_img_zero], 0)\n",
    "recon_T = utils.make_grid(recon_T, normalize=True, range=(-1, 1), nrow=recon_T.shape[0])\n",
    "display_image(recon_T, title='TrueA, ReconA')\n",
    "save_image(recon_T, 'activation_edit_T.png')\n",
    "\n",
    "recon_A = torch.cat([true_img, gen_imgA, gen_imgA_zeros], 0)\n",
    "recon_A = utils.make_grid(recon_A, normalize=True, range=(-1, 1), nrow=recon_A.shape[0])\n",
    "display_image(recon_A, title='TrueA, ReconA')\n",
    "save_image(recon_A, 'activation_edit_A.png')\n",
    "\n",
    "recon_B = torch.cat([true_img, gen_imgB, gen_imgB_zeros], 0)\n",
    "recon_B = utils.make_grid(recon_B, normalize=True, range=(-1, 1), nrow=recon_B.shape[0])\n",
    "display_image(recon_B, title='TrueB, recon_B')\n",
    "save_image(recon_B, 'activation_edit_B.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e4ca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_activation_prune_pairs(original_generator, generatorA, generatorB, generatorC):\n",
    "\n",
    "    # data_dir = 'data/multi_domain_interpol_gen'\n",
    "    data_dir = '/mnt/e0e0461d-3972-4e84-bf9b-685ecd68ebce/datasets/GAN/multi_domain_interpol_gen_10K'\n",
    "    \n",
    "    os.makedirs(f'{data_dir}/orig/', exist_ok=True)\n",
    "    os.makedirs(f'{data_dir}/z/', exist_ok=True)\n",
    "    for i in tqdm(range(10000)):\n",
    "        if os.path.exists(f'{data_dir}/z/{i}.pt'):\n",
    "            z = torch.load(f'{data_dir}/z/{i}.pt')\n",
    "        else:\n",
    "            # Sample a random z from styleGAN latent space\n",
    "            # z = torch.randn(1, latent_dim, device=device)\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for j in range(1):\n",
    "                # Generate styleA and styleB\n",
    "                img_styleA = generatorA([z], input_is_latent=False, percentile=50, mask_type='zeros')\n",
    "                img_styleB = generatorB([z], input_is_latent=False, percentile=50, mask_type='zeros')\n",
    "                img_styleC = generatorC([z], input_is_latent=False, percentile=50, mask_type='zeros')\n",
    "                \n",
    "                os.makedirs(f'{data_dir}/prune_0_pencil_sketch_{j}/', exist_ok=True)\n",
    "                os.makedirs(f'{data_dir}/prune_0_colorsketch_{j}/', exist_ok=True)\n",
    "                os.makedirs(f'{data_dir}/prune_0_watercolor_{j}/', exist_ok=True)\n",
    "                \n",
    "                img_styleA = utils.make_grid(img_styleA[0], normalize=True, range=(-1, 1))\n",
    "                img_styleB = utils.make_grid(img_styleB[0], normalize=True, range=(-1, 1))\n",
    "                img_styleC = utils.make_grid(img_styleC[0], normalize=True, range=(-1, 1))\n",
    "\n",
    "                \n",
    "                save_image(img_styleA, f'{data_dir}/prune_0_pencil_sketch_{j}/{i}.png')\n",
    "                save_image(img_styleB, f'{data_dir}/prune_0_watercolor_{j}/{i}.png')\n",
    "                save_image(img_styleC, f'{data_dir}/prune_0_colorsketch_{j}/{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8a8459",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_data_activation_prune_pairs(original_generator, generatorA, generatorB, generatorC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afe4e1f",
   "metadata": {},
   "source": [
    "## Pruning Rewind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4c3478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_activation_prune_random_pairs(original_generator, generatorA, generatorB, generatorC):\n",
    "\n",
    "    # data_dir = 'data/multi_domain_interpol_gen'\n",
    "    data_dir = '/mnt/e0e0461d-3972-4e84-bf9b-685ecd68ebce/datasets/GAN/multi_domain_interpol_gen_10k'\n",
    "    \n",
    "\n",
    "    os.makedirs(f'{data_dir}/orig/', exist_ok=True)\n",
    "    os.makedirs(f'{data_dir}/z/', exist_ok=True)\n",
    "    for i in tqdm(range(1000)):\n",
    "        if os.path.exists(f'{data_dir}/z/{i}.pt'):\n",
    "            z = torch.load(f'{data_dir}/z/{i}.pt')\n",
    "        else:\n",
    "            # Sample a random z from styleGAN latent space\n",
    "            # z = torch.randn(1, latent_dim, device=device)\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Generate styleA and styleB\n",
    "            orig_img, orig_activations = original_generator([z], input_is_latent=False, mask_type='rewind')\n",
    "            \n",
    "            img_styleA = generatorA([z], input_is_latent=False, percentile=20, mask_type='rewind', orig_activations=orig_activations)\n",
    "            img_styleB = generatorB([z], input_is_latent=False, percentile=20, mask_type='rewind', orig_activations=orig_activations)\n",
    "            img_styleC = generatorC([z], input_is_latent=False, percentile=20, mask_type='rewind', orig_activations=orig_activations)\n",
    "            \n",
    "            os.makedirs(f'{data_dir}/prune_rewind_pencil_sketch_20/', exist_ok=True)\n",
    "            os.makedirs(f'{data_dir}/prune_rewind_colorsketch_20/', exist_ok=True)\n",
    "            os.makedirs(f'{data_dir}/prune_rewind_watercolor_20/', exist_ok=True)\n",
    "            \n",
    "            img_styleA = utils.make_grid(img_styleA[0], n1ormalize=True, range=(-1, 1))\n",
    "            img_styleB = utils.make_grid(img_styleB[0], normalize=True, range=(-1, 1))\n",
    "            img_styleC = utils.make_grid(img_styleC[0], normalize=True, range=(-1, 1))\n",
    "\n",
    "            save_image(img_styleA, f'{data_dir}/prune_rewind_pencil_sketch_50/{i}.png')\n",
    "            save_image(img_styleB, f'{data_dir}/prune_rewind_watercolor_50/{i}.png')\n",
    "            save_image(img_styleC, f'{data_dir}/prune_rewind_colorsketch_50/{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9695191",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_data_activation_prune_random_pairs(original_generator, generatorA, generatorB, generatorC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "3260c0dafe9f1a95e75231d573ecdcd4e2e3cb6238b8bbda126aa1421e9f826b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
